(window.webpackJsonp=window.webpackJsonp||[]).push([[52],{562:function(t,s,a){"use strict";a.r(s);var n=a(13),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"tensorflow-训练自己的模型-自定义物件识别"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#tensorflow-训练自己的模型-自定义物件识别"}},[t._v("#")]),t._v(" tensorflow 训练自己的模型，自定义物件识别")]),t._v(" "),a("p",[t._v("上一篇文章使用了 tensorflow 提供的模型进行图像识别；这次我们将使用自己的模型进行训练并且识别。")]),t._v(" "),a("p",[t._v("目标检测是基于"),a("code",[t._v("tensorflow")]),t._v("提供的"),a("a",{attrs:{href:"https://github.com/tensorflow/models/tree/master/research/object_detection",target:"_blank",rel:"noopener noreferrer"}},[t._v("object detection API"),a("OutboundLink")],1),t._v("。整个训练的过程可以简要概括为训练集的准备和训练。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://i.imgur.com/u8tOwWq.jpg",alt:"https://i.imgur.com/u8tOwWq.jpg"}})]),t._v(" "),a("p",[a("strong",[t._v("训练集准备：")])]),t._v(" "),a("ol",[a("li",[t._v("人工标注图片，并转成"),a("code",[t._v("xml")]),t._v("格式")]),t._v(" "),a("li",[t._v("将"),a("code",[t._v("xml")]),t._v("转成"),a("code",[t._v("tf")]),t._v("能识别的数据格式即"),a("code",[t._v("tfrecord")])])]),t._v(" "),a("p",[a("strong",[t._v("训练过程：")])]),t._v(" "),a("ol",[a("li",[t._v("配置训练参数，这里需要配置\n"),a("ul",[a("li",[t._v("目标检测算法类型")]),t._v(" "),a("li",[t._v("目标类别数量")]),t._v(" "),a("li",[t._v("训练步长")]),t._v(" "),a("li",[t._v("训练部署训练集路径")]),t._v(" "),a("li",[t._v("模型输出路径")]),t._v(" "),a("li",[t._v("*以及可以基于前人训练好的模型微调")])])]),t._v(" "),a("li",[t._v("基于"),a("code",[t._v("slim")]),t._v("模块实现模型训练")])]),t._v(" "),a("h2",{attrs:{id:"一、环境配置"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#一、环境配置"}},[t._v("#")]),t._v(" 一、环境配置")]),t._v(" "),a("h3",{attrs:{id:"_1、使用-pyenv-做-python-版本切换"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1、使用-pyenv-做-python-版本切换"}},[t._v("#")]),t._v(" 1、使用 pyenv 做 Python 版本切换")]),t._v(" "),a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[t._v("    brew "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("install")]),t._v(" pyenv\n    brew "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("install")]),t._v(" pyenv-virtualenv\n    pyenv virtualenv "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.7")]),t._v(".5 object_detection_demo\n    "),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v("source")]),t._v(" ~/.bashrc\n    pyenv activate object_detection_demo\n")])])]),a("h3",{attrs:{id:"_2、下载图片标注工具"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2、下载图片标注工具"}},[t._v("#")]),t._v(" 2、下载图片标注工具")]),t._v(" "),a("p",[t._v("首先我们需要准备大量的训练集，可以针对自己的需求手动标注。我们用的是"),a("a",{attrs:{href:"https://github.com/tzutalin/labelImg",target:"_blank",rel:"noopener noreferrer"}},[t._v("labelImg"),a("OutboundLink")],1),t._v("这个 python 工具。")]),t._v(" "),a("p",[a("img",{attrs:{src:"/images/ai/self-define/1.png",alt:"/images/ai/self-define"}})]),t._v(" "),a("ol",{attrs:{start:"3"}},[a("li",[t._v("安裝 pandas")])]),t._v(" "),a("p",[t._v("Pandas 是 python 的一个数据分析库，为用户提供高效能、简易使用的资料格式，让使用者可以快速操作及分析资料")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("    pip install pandas\n")])])]),a("ol",{attrs:{start:"4"}},[a("li",[t._v("创建 workspace 和 training_demo 目录")])]),t._v(" "),a("p",[t._v("安装 tensorflow/models 源码")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("    git clone --branch r1.13.0 --depth 1 https://github.com/tensorflow/models\n")])])]),a("p",[a("img",{attrs:{src:"/images/ai/self-define/2.png",alt:"/images/ai/self-define"}})]),t._v(" "),a("h3",{attrs:{id:"_3、进行图片标注"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3、进行图片标注"}},[t._v("#")]),t._v(" 3、进行图片标注")]),t._v(" "),a("p",[t._v("此处使用的是已经标注好的图片，数据来源"),a("a",{attrs:{href:"https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10"),a("OutboundLink")],1)]),t._v(" "),a("p",[a("img",{attrs:{src:"/images/ai/self-define/3.png",alt:"/images/ai/self-define"}})]),t._v(" "),a("h3",{attrs:{id:"_4、将图片转换成-csv-格式"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4、将图片转换成-csv-格式"}},[t._v("#")]),t._v(" "),a("strong",[t._v("4、将图片转换成 csv 格式")])]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("    # 先安装pandas Pandas 是python 的一个数据分析库，为用户提供高效能、简易使用的资料格式，让使用者可以快速操作及分析资料\n    pip install pandas\n    # 启动csv文件转换\n    python src/convert_to_csv.py\n")])])]),a("div",{staticClass:"language-py extra-class"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[t._v("    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将xml转换成csv各式")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" os\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" glob\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" xml"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("etree"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ElementTree "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" ET\n\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("xml_to_csv")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        xml_list "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" xml_file "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" glob"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("glob"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'/*.xml'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            tree "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ET"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parse"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xml_file"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            root "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tree"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("getroot"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" member "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" root"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("findall"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'object'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                value "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("root"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("find"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'filename'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                         "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("root"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("find"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'size'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                         "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("root"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("find"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'size'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                         member"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                         "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("member"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                         "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("member"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                         "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("member"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                         "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("member"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                         "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                xml_list"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("value"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        column_name "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'filename'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'width'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'height'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'class'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'xmin'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'ymin'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'xmax'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'ymax'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        xml_df "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xml_list"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" columns"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("column_name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" xml_df\n\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("main")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" folder "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'train'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'test'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            image_path "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" os"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("os"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("getcwd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'images/'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" folder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            xml_df "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" xml_to_csv"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("image_path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            xml_df"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_csv"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'images/'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" folder "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'_labels.csv'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Successfully converted xml to csv.'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n    main"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h3",{attrs:{id:"_5、将-csv-转化成-tfrecord-文件"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5、将-csv-转化成-tfrecord-文件"}},[t._v("#")]),t._v(" "),a("strong",[t._v("5、将 csv 转化成 tfrecord 文件")])]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("    # 先进入models/research目录执行,不然会报错ModuleNotFoundError: No module named 'object_detection'\n    python setup.py install\n    # 如果报错：tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory\n    python src/convert_to_tfrecord.py --csv_input=images/train_labels.csv --image_dir=images/train --output_path=train.record\n    python src/convert_to_tfrecord.py --csv_input=images/test_labels.csv --image_dir=images/test --output_path=test.record\n")])])]),a("div",{staticClass:"language-py extra-class"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[t._v("    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 转换代码")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n    Usage:\n      # From tensorflow/models/\n      # Create train data:\n    \tpython src/convert_to_tfrecord.py --csv_input=images/train_labels.csv --image_dir=images/train --output_path=train.record\n\n      # Create test data:\n      python src/convert_to_tfrecord.py --csv_input=images/test_labels.csv --image_dir=images/test --output_path=test.record\n    """')]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" __future__ "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" division\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" __future__ "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" print_function\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" __future__ "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" absolute_import\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" os\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" io\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" tensorflow "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" tf\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" sys\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# sys.path.append("../models/research/object_detection")')]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" PIL "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Image\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" object_detection"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("utils "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" dataset_util\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" collections "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" namedtuple"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" OrderedDict\n\n    flags "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("app"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("flags\n    flags"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DEFINE_string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'csv_input'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Path to the CSV input'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    flags"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DEFINE_string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'image_dir'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Path to the image directory'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    flags"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DEFINE_string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'output_path'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Path to output TFRecord'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    FLAGS "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" flags"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("FLAGS\n\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# TO-DO replace this with label map")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("class_text_to_int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("row_label"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" row_label "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'nine'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("elif")]),t._v(" row_label "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'ten'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("elif")]),t._v(" row_label "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'jack'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("elif")]),t._v(" row_label "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'queen'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("elif")]),t._v(" row_label "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'king'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("elif")]),t._v(" row_label "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'ace'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("split")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" group"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" namedtuple"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'data'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'filename'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'object'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        gb "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" df"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("groupby"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("group"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("filename"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" gb"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_group"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" filename"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("zip")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("gb"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("groups"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keys"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" gb"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("groups"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("create_tf_example")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("group"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("gfile"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("GFile"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("os"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'{}'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("group"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("filename"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'rb'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" fid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            encoded_jpg "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" fid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        encoded_jpg_io "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" io"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("BytesIO"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("encoded_jpg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        image "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Image"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("encoded_jpg_io"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        width"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" height "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" image"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size\n\n        filename "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" group"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("filename"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("encode"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'utf8'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        image_format "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("b'jpg'")]),t._v("\n        xmins "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        xmaxs "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        ymins "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        ymaxs "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        classes_text "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        classes "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" row "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" group"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("object")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iterrows"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            xmins"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("row"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'xmin'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" width"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            xmaxs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("row"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'xmax'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" width"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            ymins"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("row"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'ymin'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" height"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            ymaxs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("row"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'ymax'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" height"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            classes_text"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("row"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'class'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("encode"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'utf8'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            classes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("class_text_to_int"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("row"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'class'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        tf_example "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Example"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("features"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Features"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("feature"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'image/height'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" dataset_util"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("int64_feature"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("height"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'image/width'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" dataset_util"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("int64_feature"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("width"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'image/filename'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" dataset_util"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bytes_feature"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("filename"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'image/source_id'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" dataset_util"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bytes_feature"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("filename"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'image/encoded'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" dataset_util"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bytes_feature"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("encoded_jpg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'image/format'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" dataset_util"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bytes_feature"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("image_format"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'image/object/bbox/xmin'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" dataset_util"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("float_list_feature"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xmins"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'image/object/bbox/xmax'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" dataset_util"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("float_list_feature"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xmaxs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'image/object/bbox/ymin'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" dataset_util"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("float_list_feature"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ymins"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'image/object/bbox/ymax'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" dataset_util"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("float_list_feature"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ymaxs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'image/object/class/text'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" dataset_util"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bytes_list_feature"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("classes_text"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'image/object/class/label'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" dataset_util"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("int64_list_feature"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("classes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" tf_example\n\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("main")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("_"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        writer "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("python_io"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("TFRecordWriter"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("FLAGS"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("output_path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        path "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" os"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("os"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("getcwd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" FLAGS"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("image_dir"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        examples "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("FLAGS"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("csv_input"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        grouped "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" split"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("examples"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'filename'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" group "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" grouped"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            tf_example "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" create_tf_example"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("group"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            writer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("write"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf_example"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("SerializeToString"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        writer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("close"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        output_path "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" os"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("os"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("getcwd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" FLAGS"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("output_path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Successfully created the TFRecords: {}'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output_path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" __name__ "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'__main__'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("app"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h2",{attrs:{id:"二、训练准备"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#二、训练准备"}},[t._v("#")]),t._v(" 二、训练准备")]),t._v(" "),a("h3",{attrs:{id:"_1、设置-labelmap-pbtxt"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1、设置-labelmap-pbtxt"}},[t._v("#")]),t._v(" "),a("strong",[t._v("1、设置 labelmap.pbtxt")])]),t._v(" "),a("p",[t._v("训练前我们需要配置检测目标的类别，在 config/labelmap 中定义")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("    item {\n      id: 1\n      name: 'nine'\n    }\n\n    item {\n      id: 2\n      name: 'ten'\n    }\n\n    item {\n      id: 3\n      name: 'jack'\n    }\n\n    item {\n      id: 4\n      name: 'queen'\n    }\n\n    item {\n      id: 5\n      name: 'king'\n    }\n\n    item {\n      id: 6\n      name: 'ace'\n    }\n")])])]),a("h3",{attrs:{id:"_2、选择需要训练的模型"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2、选择需要训练的模型"}},[t._v("#")]),t._v(" "),a("strong",[t._v("2、选择需要训练的模型")])]),t._v(" "),a("p",[t._v("此处选择的是 faster_rcnn_inception_v2_coco_2018_01_28 模型；下载地址："),a("a",{attrs:{href:"http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_v2_coco_2018_01_28.tar.gz",target:"_blank",rel:"noopener noreferrer"}},[t._v("http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_v2_coco_2018_01_28.tar.gz"),a("OutboundLink")],1)]),t._v(" "),a("p",[t._v("所有的模型列表请见此处："),a("a",{attrs:{href:"https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"),a("OutboundLink")],1)]),t._v(" "),a("p",[a("img",{attrs:{src:"/images/ai/self-define/4.png",alt:"/images/ai/self-define"}})]),t._v(" "),a("h3",{attrs:{id:"_3、创建并修改模型配置文件"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3、创建并修改模型配置文件"}},[t._v("#")]),t._v(" 3、创建并修改模型配置文件")]),t._v(" "),a("p",[t._v("需要修改的点是：")]),t._v(" "),a("ul",[a("li",[t._v("fine_tune_checkpoint: 下载的预训练模型路径/model.ckpt")]),t._v(" "),a("li",[t._v('iput_path: "/预处理数据生成的 tfrecords 格式数据的文件路径”，分为训练集和验证集两个；')]),t._v(" "),a("li",[t._v("label_map_path: “/格式转换过程中使用过的类别与 ID 对应的 pbtxt 文件”")]),t._v(" "),a("li",[t._v("num_classes: 我们自己的数据集的类别数")])]),t._v(" "),a("p",[t._v("同时在这个配置文件中还可以更改训练时的 batch_size,学习率，epoch 数量，数据增强的方式，优化算法的选择，评价指标等。")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v('    # Faster R-CNN with Inception v2, configured for Oxford-IIIT Pets Dataset.\n    # Users should configure the fine_tune_checkpoint field in the train config as\n    # well as the label_map_path and input_path fields in the train_input_reader and\n    # eval_input_reader. Search for "PATH_TO_BE_CONFIGURED" to find the fields that\n    # should be configured.\n\n    model {\n      faster_rcnn {\n        num_classes: 6\n        image_resizer {\n          keep_aspect_ratio_resizer {\n            min_dimension: 600\n            max_dimension: 1024\n          }\n        }\n        feature_extractor {\n          type: \'faster_rcnn_inception_v2\'\n          first_stage_features_stride: 16\n        }\n        first_stage_anchor_generator {\n          grid_anchor_generator {\n            scales: [0.25, 0.5, 1.0, 2.0]\n            aspect_ratios: [0.5, 1.0, 2.0]\n            height_stride: 16\n            width_stride: 16\n          }\n        }\n        first_stage_box_predictor_conv_hyperparams {\n          op: CONV\n          regularizer {\n            l2_regularizer {\n              weight: 0.0\n            }\n          }\n          initializer {\n            truncated_normal_initializer {\n              stddev: 0.01\n            }\n          }\n        }\n        first_stage_nms_score_threshold: 0.0\n        first_stage_nms_iou_threshold: 0.7\n        first_stage_max_proposals: 300\n        first_stage_localization_loss_weight: 2.0\n        first_stage_objectness_loss_weight: 1.0\n        initial_crop_size: 14\n        maxpool_kernel_size: 2\n        maxpool_stride: 2\n        second_stage_box_predictor {\n          mask_rcnn_box_predictor {\n            use_dropout: false\n            dropout_keep_probability: 1.0\n            fc_hyperparams {\n              op: FC\n              regularizer {\n                l2_regularizer {\n                  weight: 0.0\n                }\n              }\n              initializer {\n                variance_scaling_initializer {\n                  factor: 1.0\n                  uniform: true\n                  mode: FAN_AVG\n                }\n              }\n            }\n          }\n        }\n        second_stage_post_processing {\n          batch_non_max_suppression {\n            score_threshold: 0.0\n            iou_threshold: 0.6\n            max_detections_per_class: 100\n            max_total_detections: 300\n          }\n          score_converter: SOFTMAX\n        }\n        second_stage_localization_loss_weight: 2.0\n        second_stage_classification_loss_weight: 1.0\n      }\n    }\n\n    train_config: {\n      batch_size: 1\n      optimizer {\n        momentum_optimizer: {\n          learning_rate: {\n            manual_step_learning_rate {\n              initial_learning_rate: 0.0002\n              schedule {\n                step: 1\n                learning_rate: .0002\n              }\n              schedule {\n                step: 900000\n                learning_rate: .00002\n              }\n              schedule {\n                step: 1200000\n                learning_rate: .000002\n              }\n            }\n          }\n          momentum_optimizer_value: 0.9\n        }\n        use_moving_average: false\n      }\n      gradient_clipping_by_norm: 10.0\n      fine_tune_checkpoint: "/Users/alexganggao/Documents/Study/tensorflow_detection/model/faster_rcnn_inception_v2_coco_2018_01_28/model.ckpt"\n      from_detection_checkpoint: true\n      # Note: The below line limits the training process to 200K steps, which we\n      # empirically found to be sufficient enough to train the pets dataset. This\n      # effectively bypasses the learning rate schedule (the learning rate will\n      # never decay). Remove the below line to train indefinitely.\n      num_steps: 1000\n      data_augmentation_options {\n        random_horizontal_flip {\n        }\n      }\n    }\n\n\n    train_input_reader: {\n      tf_record_input_reader {\n        input_path: "/Users/alexganggao/Documents/Study/tensorflow_detection/train.record"\n      }\n      label_map_path: "/Users/alexganggao/Documents/Study/tensorflow_detection/config/labelmap.pbtxt"\n    }\n\n    eval_config: {\n      num_examples: 67\n      # Note: The below line limits the evaluation process to 10 evaluations.\n      # Remove the below line to evaluate indefinitely.\n      max_evals: 10\n    }\n\n    eval_input_reader: {\n      tf_record_input_reader {\n        input_path: "/Users/alexganggao/Documents/Study/tensorflow_detection/test.record"\n      }\n      label_map_path: "/Users/alexganggao/Documents/Study/tensorflow_detection/config/labelmap.pbtxt"\n      shuffle: false\n      num_readers: 1\n    }\n')])])]),a("h3",{attrs:{id:"_4、编写训练脚本-开始训练模型"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4、编写训练脚本-开始训练模型"}},[t._v("#")]),t._v(" 4、编写训练脚本，开始训练模型")]),t._v(" "),a("p",[t._v("slim 是干嘛的？")]),t._v(" "),a("p",[t._v("TF-slim 是 TensorFlow 的新轻量级高级 API（tensorflow.contrib.slim），用于定义，训练和评估复杂模型。 该目录包含用于训练和评估使用 TF-slim 的几种广泛使用的卷积神经网络（CNN）图像分类模型的代码。 它包含脚本，使您可以从头开始训练模型或从预先训练的网络权重中微调模型。 它还包含用于下载标准图像数据集，将其转换为 TensorFlow 的本机 TFRecord 格式并使用 TF-Slim 的数据读取和排队实用程序进行读取的代码")]),t._v(" "),a("p",[t._v("如何使用 slim:")]),t._v(" "),a("ul",[a("li",[t._v("在 models/research 下面执行 "),a("code",[t._v("export PYTHONPATH=$PYTHONPATH:'pwd':'pwd'/slim")])]),t._v(" "),a("li",[t._v("拷贝 slim 到工程根目录，并执行")])]),t._v(" "),a("p",[a("code",[t._v("export PYTHONPATH=$PYTHONPATH:'pwd':'pwd'/slim")])]),t._v(" "),a("p",[t._v("或者在代码中引入")]),t._v(" "),a("div",{staticClass:"language-py extra-class"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[t._v("    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将 slim 添加到查找路径中")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("add_slim_to_path")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        slim_path "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" os"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("abspath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("os"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("os"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dirname"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("__file__"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'../slim'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        sys"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("slim_path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[t._v("    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 启动模型训练")]),t._v("\n    python src/train.py\n")])])]),a("p",[t._v("如果出现如下图则表示模型正在训练")]),t._v(" "),a("p",[a("img",{attrs:{src:"/images/ai/self-define/5.png",alt:"/images/ai/self-define"}})]),t._v(" "),a("p",[t._v("我们可以在控制台执行，查看实时训练进展")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("    # logdir为模型训练的结果地址：TRAIN_DATA_PATH\n    tensorboard --logdir=train_data/faster\n")])])]),a("p",[a("img",{attrs:{src:"/images/ai/self-define/6.png",alt:"/images/ai/self-define"}})]),t._v(" "),a("p",[t._v("等待很久很久的时间显示训练完成（此处只设置了训练步长为 1000）")]),t._v(" "),a("p",[a("img",{attrs:{src:"/images/ai/self-define/7.png",alt:"/images/ai/self-define"}})]),t._v(" "),a("h3",{attrs:{id:"_5、模型导出"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5、模型导出"}},[t._v("#")]),t._v(" 5、模型导出")]),t._v(" "),a("p",[t._v("这一步的前提是：上一步模型训练完成")]),t._v(" "),a("p",[t._v("我们此时需要对训练完成的数据进行模型导出。"),a("code",[t._v("tf object detection")]),t._v("也提供了相应的"),a("code",[t._v("api")]),t._v("，具体文件路径是 t"),a("code",[t._v("f/model/object_detection/export_inference_graph.py")])]),t._v(" "),a("p",[t._v("使用提供的框架可以直接调用")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("    python src/export.py\n")])])]),a("p",[t._v("执行完成之后发现工程目录下存在了：")]),t._v(" "),a("p",[a("img",{attrs:{src:"/images/ai/self-define/8.png",alt:"/images/ai/self-define"}})]),t._v(" "),a("p",[t._v("其中标红处就是我们训练的模型。")]),t._v(" "),a("h3",{attrs:{id:"_6、模型检测"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6、模型检测"}},[t._v("#")]),t._v(" 6、模型检测")]),t._v(" "),a("p",[t._v("在得到了训练好的模型之后，我们就能用它对我们的视图进行目标检测了。")]),t._v(" "),a("p",[t._v("检测这边"),a("code",[t._v("tf")]),t._v("提供的目标代码是"),a("code",[t._v("models/research/object_detection/object_detection_tutorial.py")]),t._v("可以自行参考，根据自己需求改写。")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("    python src/detection.py\n")])])]),a("p",[t._v("可以看到工程下出现了检测结果：")]),t._v(" "),a("p",[a("img",{attrs:{src:"/images/ai/self-define/9.png",alt:"/images/ai/self-define"}})]),t._v(" "),a("h3",{attrs:{id:"_7、模型评估"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_7、模型评估"}},[t._v("#")]),t._v(" 7、模型评估")]),t._v(" "),a("p",[t._v("单我们单纯跑一张图预测的话其实还不足以评估模型的准确性，所以"),a("code",[t._v("tf")]),t._v("也提供了相应模型评估的函数。")]),t._v(" "),a("p",[t._v("具体可以参考"),a("code",[t._v("tf/model/object_detection/legacy/eval")])]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("    python src/eval.py\n")])])]),a("p",[t._v("其实就是使用我们在模型"),a("code",[t._v("config/faster_rcnn_inception_v2_pets.config")]),t._v("中配置 的"),a("code",[t._v("eval_input_reader.tf_record_input_reader")]),t._v("中的测试集数据去测试的。控制台会输出该模型下每个类别的检测准确率。")]),t._v(" "),a("p",[a("img",{attrs:{src:"/images/ai/self-define/10.png",alt:"/images/ai/self-define"}})]),t._v(" "),a("p",[t._v("可能出现的问题：")]),t._v(" "),a("p",[t._v("NameError: name 'unicode' is not defined")]),t._v(" "),a("p",[t._v("解决办法：")]),t._v(" "),a("p",[t._v("Python2 的 unicode 函数在 Python3 中被命名为 str。在 Python3 中使用 ·str 来代替 Python2 中的 unicode.")]),t._v(" "),a("h2",{attrs:{id:"参考"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#参考"}},[t._v("#")]),t._v(" 参考")]),t._v(" "),a("ul",[a("li",[a("a",{attrs:{href:"https://kenlwk.github.io/3_CustomObjectReconition#%E4%BB%8B%E7%B4%B9",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://kenlwk.github.io/3_CustomObjectReconition#介紹"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://segmentfault.com/a/1190000022286170",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://segmentfault.com/a/1190000022286170"),a("OutboundLink")],1)])])])}),[],!1,null,null,null);s.default=e.exports}}]);